{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and Evaluating Models\n",
    "Based on the small number of variables and the fact that this is a regression problem, I decided to keep the models fairly simple. In the pairplots in the [EDA and cleaning notebook](./02_eda_and_cleaning.ipynb), I observed what appeared to be a polynomial relationship between the magnitudes in each band (U, G, R, I, Z) and the redshift, so I tested a polynomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "# import modules for preprocessing and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = pd.read_csv(\"./data/sdss_clean.csv\", index_col=False)\n",
    "# read in cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = galaxies[[\"u\", \"g\", \"r\", \"i\", \"z\"]]\n",
    "y = galaxies[\"redshift\"]\n",
    "# create feature matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=20200310)\n",
    "# split X and y into training and test sets for model fitting and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set baseline (RMSE): 0.2226269235953162\n",
      "Test set baseline (RMSE): 0.2244908042024483\n"
     ]
    }
   ],
   "source": [
    "y_train_mean = y_train.mean()\n",
    "y_train_baseline = list()\n",
    "for i in range(len(y_train)):\n",
    "    y_train_baseline.append(y_train_mean)\n",
    "\n",
    "y_test_mean = y_test.mean()\n",
    "y_test_baseline = list()\n",
    "for i in range(len(y_test)):\n",
    "    y_test_baseline.append(y_test_mean)\n",
    "\n",
    "baseline_train_rmse = np.sqrt(MSE(y_train, y_train_baseline))\n",
    "baseline_test_rmse = np.sqrt(MSE(y_test, y_test_baseline))\n",
    "\n",
    "print(f\"Training set baseline (RMSE): {baseline_train_rmse}\")\n",
    "print(f\"Test set baseline (RMSE): {baseline_test_rmse}\")\n",
    "# for both training and test set: obtain mean squared error for a model that predicts the mean of y for every observation, then take the square root\n",
    "# to obtain the RMSE and use this as the baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train);\n",
    "# instantiate a linear regression and fit it to X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set r2 score: 0.8392073178821228\n",
      "Test set r2 score: 0.8315979055902201\n",
      "Training set RMSE: 0.08919968190914797\n",
      "Trest set RMSE: 0.09201922483354388\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set r2 score: {cross_val_score(linreg, X_train, y_train).mean()}\")\n",
    "print(f\"Test set r2 score: {cross_val_score(linreg, X_test, y_test).mean()}\")\n",
    "print(f\"Training set RMSE: {np.sqrt(MSE(y_train, linreg.predict(X_train)))}\")\n",
    "print(f\"Test set RMSE: {np.sqrt(MSE(y_test, linreg.predict(X_test)))}\")\n",
    "# get the average R2 score on the training and test set using 5-fold cross validation as well as RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_poly = PolynomialFeatures(2)\n",
    "# instantiate a polynomial features transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quad = quad_poly.fit_transform(X_train)\n",
    "X_test_quad = quad_poly.transform(X_test)\n",
    "# fit to and transform the training set features; transform the test set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_2 = LinearRegression()\n",
    "linreg_2.fit(X_quad_train, y_train);\n",
    "# instantiate a new linear regression and fit it to the transformed training set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set r2 score: 0.8577803027139433\n",
      "Test set r2 score: 0.8538380742950643\n",
      "Training set RMSE: 0.08318758776911535\n",
      "Test set RMSE: 0.0857170064855389\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set r2 score: {cross_val_score(linreg_2, X_train_quad, y_train).mean()}\")\n",
    "print(f\"Test set r2 score: {cross_val_score(linreg_2, X_test_quad, y_test).mean()}\")\n",
    "print(f\"Training set RMSE: {np.sqrt(MSE(y_train, linreg_2.predict(X_quad_train)))}\")\n",
    "print(f\"Test set RMSE: {np.sqrt(MSE(y_test, linreg_2.predict(X_quad_test)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cubic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_poly = PolynomialFeatures(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cube = cube_poly.fit_transform(X_train)\n",
    "X_test_cube = cube_poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_3 = LinearRegression()\n",
    "linreg_3.fit(X_cube_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set r2 score: 0.8033156511454067\n",
      "Test set r2 score: 0.8381467722433819\n",
      "Training set RMSE: 0.07914729674451988\n",
      "Test set RMSE: 0.084846273370319\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set r2 score: {cross_val_score(linreg_3, X_train_cube, y_train).mean()}\")\n",
    "print(f\"Test set r2 score: {cross_val_score(linreg_3, X_test_cube, y_test).mean()}\")\n",
    "print(f\"Training set RMSE: {np.sqrt(MSE(y_train, linreg_3.predict(X_cube_train)))}\")\n",
    "print(f\"Test set RMSE: {np.sqrt(MSE(y_test, linreg_3.predict(X_cube_test)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "21\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1])\n",
    "print(X_train_quad.shape[1])\n",
    "print(X_train_cube.shape[1])\n",
    "# number of features used for linear, quadratic and cubic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that the quadratic regression is the strongest of these three models. It outperformed the linear regression on both metrics; the cubic regression saw a very slight reduction in RMSE over the quadratic, but this was offset by a reduction in r<sup>2</sup> scores as well as a large increase in the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# instantiate transformer to scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "# scale training set and test set features to enable KNN model to make good predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set R2 score: 0.8674301716149604\n",
      "Test set R2 score: 0.8518395208802725\n",
      "Training set RMSE: 0.06552448183077234\n",
      "Test set RMSE: 0.0823038157725722\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "# instantiate KNN regression with default 5 neighbors used\n",
    "knn.fit(X_train_sc, y_train);\n",
    "print(f\"Training set R2 score: {cross_val_score(knn, X_train_sc, y_train).mean()}\")\n",
    "print(f\"Test set R2 score: {cross_val_score(knn, X_test_sc, y_test).mean()}\")\n",
    "print(f\"Training set RMSE: {np.sqrt(MSE(y_train, knn.predict(X_train_sc)))}\")\n",
    "print(f\"Test set RMSE: {np.sqrt(MSE(y_test, knn.predict(X_test_sc)))}\")\n",
    "# print r2 and RMSE on training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores = list()\n",
    "rmse = list()\n",
    "# empty lists to hold evaluation metrics for 30 KNN models\n",
    "\n",
    "for k in range(1, 31):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train_sc, y_train);\n",
    "    r2_scores.append((cross_val_score(knn, X_test_sc, y_test).mean(), k))\n",
    "    rmse.append((np.sqrt(MSE(y_test, knn.predict(X_test_sc))), k))\n",
    "    # instantiate and fit 30 KNN models with k=1 to k=30 neighbors; put the scores and k values in the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores.sort(reverse=True)\n",
    "# sort r2 scores from highest to lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse.sort()\n",
    "# sort RMSE values from lowest to highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8588578414486603, 10),\n",
       " (0.8588010539775712, 8),\n",
       " (0.8587968020869482, 12),\n",
       " (0.8587603754409912, 9),\n",
       " (0.8587461021559388, 16)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_scores[:5]\n",
    "# see 5 best r2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.08003349292320475, 15),\n",
       " (0.08009857182535854, 14),\n",
       " (0.08010869826868235, 17),\n",
       " (0.08012373861161977, 16),\n",
       " (0.0801550811947784, 13)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse[:5]\n",
    "# see 5 best RMSE scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 16 appears in both the 5 highest r<sup>2</sup> scores and the 5 lowest RMSE values. The KNN model using 16 neighbors gave an r<sup>2</sup> score of 0.859 and an RMSE of 0.080 on the test set. This is a very slight improvement over the quadratic regression. The fact that no values above 17 show up in either of these lists suggests that 16 is the optimal number, or at least a good tradeoff between performance and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=20200310)\n",
    "# instantiate random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_params = {\n",
    "    \"n_estimators\": [50, 100]\n",
    "}\n",
    "# create dictionary of hyperparemeters for gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    estimator=rfr,\n",
    "    param_grid=rfr_params\n",
    ")\n",
    "# instantiate gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.879315496955938"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07798737615105548"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(MSE(y_test, gs.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to test more models, but the random forest regression takes a very long time to train and I need to submit this for checkin 4. I will run more models (perhaps including other ensemble models) onWednesday and hopefully push that score up a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
